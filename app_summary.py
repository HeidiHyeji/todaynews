import requests
import streamlit as st
import json
from datetime import datetime, timedelta
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# KoBART ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ í•¨ìˆ˜
@st.cache_resource  # Streamlitì—ì„œ ëª¨ë¸ì„ ìºì‹±í•˜ì—¬ ë¡œë“œ ì†ë„ ê°œì„ 
def load_kobart_model():
    model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')
    tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')
    return model, tokenizer

# ì¶”ì¶œì  ìš”ì•½ í•¨ìˆ˜
def extractive_summary(content, n=3):
    """
    ì¶”ì¶œì  ìš”ì•½: TF-IDF ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” ë¬¸ì¥ nê°œ ì¶”ì¶œ
    """
    sentences = content.split('. ')
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)
    sentence_scores = tfidf_matrix.sum(axis=1).flatten().tolist()[0]
    ranked_sentences = np.argsort(sentence_scores)[::-1][:n]  # ìƒìœ„ nê°œ ë¬¸ì¥ ì„ íƒ
    extracted_summary = '. '.join([sentences[i] for i in ranked_sentences])
    return extracted_summary

# ìƒì„±ì  ìš”ì•½ í•¨ìˆ˜
def generative_summary(content, model, tokenizer, max_length=64, min_length=16):
    """
    KoBART ìƒì„±ì  ìš”ì•½
    """
    inputs = tokenizer.encode("summarize: " + content, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(
        inputs,
        max_length=max_length,
        min_length=min_length,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# í•˜ì´ë¸Œë¦¬ë“œ ìš”ì•½ í•¨ìˆ˜
def hybrid_summary(content, model, tokenizer, extract_n=3):
    """
    ì¶”ì¶œì  ìš”ì•½ + ìƒì„±ì  ìš”ì•½ì„ ê²°í•©
    """
    extracted = extractive_summary(content, n=extract_n)
    generated = generative_summary(extracted, model, tokenizer)
    return generated

# ì˜¤ëŠ˜ ë‚ ì§œì™€ ë‚´ì¼ ë‚ ì§œ ê³„ì‚°
today = datetime.today()
tomorrow = today + timedelta(days=1)

# ë‚ ì§œë¥¼ 'YYYY-MM-DD' í˜•ì‹ìœ¼ë¡œ ë³€í™˜
from_date = today.strftime('%Y-%m-%d')
until_date = tomorrow.strftime('%Y-%m-%d')

# ê¸°ë³¸ API ìš”ì²­ ë§¤ê°œë³€ìˆ˜
params = {
    "argument": {
        "query": "ë¡¯ë°ì´ë…¸ë² ì´íŠ¸ OR ì´ë…¸ë² ì´íŠ¸ OR ë¡¯ë° AND AI",  # ì´ˆê¸° query (ë³€ê²½ ê°€ëŠ¥)
        "published_at": {
            "from": from_date,
            "until": until_date
        },
        "category": [
            "008000000"
        ],
        "sort": {
            "date": "desc"
        },
        "return_from": "0",
        "return_size": "10000",
        "fields": [
            "news_id",
            "published_at",
            "title",
            "content",
            "provider",
            "byline",
            "category",
            "category_incident",
            "provider_link_page",
            "printing_page"
        ]
    },
    "access_key": "newstoresample"
}

headers = {'Content-type': 'application/json'}

# Streamlitì—ì„œ ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ì›Œë“œ ë°›ì•„ì˜¤ê¸°
st.markdown(f"<h1 style='color: rgb(237, 27, 36); text-align: center;'>ğŸ“° ì˜¤ëŠ˜ì˜ ë¡¯ë° <span style='font-size: 14px; color: black;'>- {from_date}</span></h1>", unsafe_allow_html=True)

# ê¸°ë³¸ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ëŠ” í•„ë“œ (ì´ˆê¸° queryë¬¸ì— í¬í•¨)
default_keywords = ["ë¡¯ë°ì´ë…¸ë² ì´íŠ¸", "ì´ë…¸ë² ì´íŠ¸", "ë¡¯ë° AND AI"]
keywords_input = st.text_input("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (separate by ì½¤ë§ˆ, AND)", ", ".join(default_keywords))

# ì…ë ¥ëœ í‚¤ì›Œë“œì— ë”°ë¼ query ë³€ê²½
if keywords_input:
    keywords = [keyword.strip() for keyword in keywords_input.split(",")]
    query = " OR ".join(keywords)
else:
    query = "ë¡¯ë°ì´ë…¸ë² ì´íŠ¸ OR ì´ë…¸ë² ì´íŠ¸ OR ë¡¯ë° AND AI"

# API ìš”ì²­ ë§¤ê°œë³€ìˆ˜ì— query ë°˜ì˜
params["argument"]["query"] = query

# KoBART ëª¨ë¸ ë¡œë“œ
model, tokenizer = load_kobart_model()

# API ìš”ì²­ì„ ë³´ë‚´ê³  ê²°ê³¼ë¥¼ ë°›ìŒ
response = requests.post("https://www.newstore.or.kr/api-newstore/v1/search/newsAllList.json", json=params, headers=headers)

# ì‘ë‹µì´ ì •ìƒì ì¸ ê²½ìš°
if response.status_code == 200:
    result = response.json()
    
    # 'returnObject'ê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë‹¤ì‹œ JSONìœ¼ë¡œ íŒŒì‹±
    return_object = json.loads(result.get('returnObject', '{}'))
    
    # 'documents' ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = return_object.get('documents', [])
    
    # ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    if documents:
        st.write(f"ğŸ“¢ ì´ {len(documents)}ê±´ì˜ ì˜¤ëŠ˜ì˜ ë¡¯ë° ì†Œì‹ì„ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤! ")
        
        for document in documents:
            title = document.get('title', 'No Title')
            content = document.get('content', 'No Content')
            provider = document.get('provider', 'No Provider')
            provider_link_page = document.get('provider_link_page', 'No Link')
            published_at = document.get('published_at', 'No Date')

            # ë‚ ì§œ í¬ë§· ë³€ê²½: ISO 8601 í˜•ì‹ì—ì„œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            published_at_date = datetime.fromisoformat(published_at.replace("Z", "+00:00"))
            formatted_date = published_at_date.strftime('%Y-%m-%d %H:%M:%S')

            # í•˜ì´ë¸Œë¦¬ë“œ ìš”ì•½ ì‹¤í–‰
            summarized_content = hybrid_summary(content, model, tokenizer)

            # ì œëª©ì„ í´ë¦­í•˜ë©´ ë§í¬ë¡œ ì´ë™ (ì œëª©ì€ êµµì€ ê¸€ì”¨ë¡œ í‘œì‹œ)
            with st.expander(f"**{title}**"):
                # ìš”ì•½ëœ ë‰´ìŠ¤ ë‚´ìš© í‘œì‹œ
                st.write(f"**ìš”ì•½ ë‚´ìš©:** {summarized_content}")

                # ì „ì²´ ë‰´ìŠ¤ ë‚´ìš© í‘œì‹œ
                st.write(f"**ì „ì²´ ë‰´ìŠ¤:** {content}")
                
                # providerì™€ ë§í¬ë¥¼ í•œ ì¤„ì— í‘œì‹œ
                st.markdown(f'<div style="display: flex; justify-content: space-between; align-items: center;">'
                            f'<span>Provider: {provider}</span>'
                            f'<a href="{provider_link_page}" target="_blank" style="text-decoration: none;">[Go to link]</a>'
                            f'</div>', unsafe_allow_html=True)

                # ë‚ ì§œ í‘œì‹œ
                st.markdown(f"**Published at: {formatted_date}**", unsafe_allow_html=True)
    else:
        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ "ë¬´ì†Œì‹ì´ í¬ì†Œì‹ì…ë‹ˆë‹¤!" ì¶œë ¥
        st.write("ë¬´ì†Œì‹ì´ í¬ì†Œì‹ì…ë‹ˆë‹¤!ğŸ˜Œ")
else:
    # ì‘ë‹µì´ ì‹¤íŒ¨í•œ ê²½ìš°
    st.error("Failed to fetch data from the API.")
